{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matop(*matrices, separator=\"\\t\\t\", width=5):\n",
    "    \"\"\"\n",
    "    Prints multiple matrices side by side, ensuring proper alignment.\n",
    "    \n",
    "    Args:\n",
    "    - matrices: The matrices to print side by side.\n",
    "    - separator: The string used to separate the matrices in the output.\n",
    "    - width: The fixed width for each element in the matrix to ensure alignment.\n",
    "    \"\"\"\n",
    "    # Find the maximum number of rows across all matrices\n",
    "    max_rows = max(len(matrix) for matrix in matrices)\n",
    "    \n",
    "    # Iterate over the rows by index up to the max number of rows\n",
    "    for i in range(max_rows):\n",
    "        formatted_rows = []\n",
    "        for matrix in matrices:\n",
    "            # Check if the matrix has a row at index i\n",
    "            if i < len(matrix):\n",
    "                row = matrix[i]\n",
    "                # If row is iterable (like a list or numpy array), format each element\n",
    "                if hasattr(row, '__iter__'):\n",
    "                    formatted_rows.append(\"[\" + \" \".join(f\"{val:{width}.2f}\" for val in row) + \"]\")\n",
    "                else:\n",
    "                    # If it's a single scalar value, format it directly\n",
    "                    formatted_rows.append(f\"  {row:{width}.2f}\")\n",
    "            else:\n",
    "                # Add an empty value if the matrix does not have enough rows\n",
    "                formatted_rows.append('\\t' * (width // 4))\n",
    "        \n",
    "        # Join the formatted rows with the specified separator and print them\n",
    "        print(separator.join(formatted_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bc: 3\n",
      "Br: 2\n",
      "Q \t\t K \t\t V\n",
      "[-0.47  1.09] \t [-2.03  0.19] \t [1.77 0.4 ]\n",
      "[ 0.06 -1.08] \t [-0.66  0.85] \t [-1.26  0.92]\n",
      "[-0.72  0.68] \t [-0.79 -0.11] \t [2.12 1.03]\n",
      "[-0.73  0.22] \t [0.5  0.87] \t [-1.52 -0.48]\n",
      "[ 0.05 -0.65] \t [-1.2  -0.33] \t [ 1.27 -0.71]\n",
      "[2.14 0.63] \t [-0.47 -0.65] \t [0.44 0.77]\n"
     ]
    }
   ],
   "source": [
    "# SRAM memory size\n",
    "M = 20\n",
    "# simplified head size = hidden size\n",
    "d = 2\n",
    "# Sequence length\n",
    "N = 6\n",
    "\n",
    "# set block size for outer loop\n",
    "Bc = math.ceil(M / (4 * d))\n",
    "print(f\"Bc: {Bc}\")\n",
    "# set block size for inner loop\n",
    "Br = min(Bc, d)\n",
    "print(f\"Br: {Br}\")\n",
    "\n",
    "# Example dimensions for Q, K, V matrices\n",
    "Q = np.random.randn(N, d)\n",
    "K = np.random.randn(N, d)\n",
    "V = np.random.randn(N, d)\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Q \\t\\t K \\t\\t V\")\n",
    "    for q, k, v in zip(Q, K, V):\n",
    "        print(f\"{q} \\t {k} \\t {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Forward Pass\n",
    "- direct attention computation using numpy/scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output O:\n",
      "[[ 0.23  0.37]\n",
      " [ 0.85  0.36]\n",
      " [ 0.69  0.38]\n",
      " [ 0.91  0.35]\n",
      " [ 0.74  0.35]\n",
      " [-1.28 -0.29]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax  # Optional, can use numpy's method or scipy's\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    # Step 1: Calculate the dot product of Q and K.T (scores)\n",
    "    S = np.dot(Q, K.T)  # (N, d) @ (d, N) -> (N, N)\n",
    "    \n",
    "    # Step 2: Apply the softmax to the scores for each row\n",
    "    P = softmax(S, axis=1)  # Softmax along the rows\n",
    "    \n",
    "    # Step 3: Multiply the attention weights with the value matrix V\n",
    "    O = np.dot(P, V)  # (N, N) @ (N, d) -> (N, d)\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Calculate the attention output\n",
    "_O = attention(Q, K, V)\n",
    "\n",
    "# Print the result\n",
    "print(\"Attention Output O:\")\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(_O)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward + Backward Pass\n",
    "\n",
    "### Simplified Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "  S \t\t\t=\t\t\t Q \t\t@\t\t K.T\n",
      "[ 1.16  1.24  0.25  0.70  0.20 -0.49]\t\t[-0.47  1.09]\t\t[-2.03 -0.66 -0.79  0.50 -1.20 -0.47]\n",
      "[-0.33 -0.96  0.07 -0.90  0.28  0.67]\t\t[ 0.06 -1.08]\t\t[ 0.19  0.85 -0.11  0.87 -0.33 -0.65]\n",
      "[ 1.58  1.05  0.49  0.23  0.63 -0.10]\t\t[-0.72  0.68]\t\t\t\n",
      "[ 1.52  0.67  0.55 -0.18  0.80  0.21]\t\t[-0.73  0.22]\t\t\t\n",
      "[-0.21 -0.59  0.04 -0.54  0.16  0.40]\t\t[ 0.05 -0.65]\t\t\t\n",
      "[-4.22 -0.88 -1.77  1.63 -2.79 -1.43]\t\t[ 2.14  0.63]\t\t\t\n",
      "  A \t\t\t=\t\t\t\t exp(S - m) \t\t\t/\t\t sum(exp(S - m))\n",
      "[ 0.27  0.29  0.11  0.17  0.10  0.05]\t\t\t[ 0.92  1.00  0.37  0.59  0.35  0.18]\t\t\t[ 3.41]\n",
      "[ 0.12  0.07  0.18  0.07  0.23  0.33]\t\t\t[ 0.37  0.19  0.55  0.21  0.68  1.00]\t\t\t[ 2.99]\n",
      "[ 0.36  0.21  0.12  0.09  0.14  0.07]\t\t\t[ 1.00  0.59  0.34  0.26  0.39  0.19]\t\t\t[ 2.77]\n",
      "[ 0.36  0.16  0.14  0.07  0.18  0.10]\t\t\t[ 1.00  0.43  0.38  0.18  0.49  0.27]\t\t\t[ 2.75]\n",
      "[ 0.14  0.10  0.18  0.10  0.21  0.26]\t\t\t[ 0.54  0.37  0.69  0.39  0.79  1.00]\t\t\t[ 3.78]\n",
      "[ 0.00  0.07  0.03  0.85  0.01  0.04]\t\t\t[ 0.00  0.08  0.03  1.00  0.01  0.05]\t\t\t[ 1.18]\n",
      "  O \t\t=\t\t\t A \t\t\t@\t\t V\n",
      "[ 0.23  0.37]\t\t[ 0.27  0.29  0.11  0.17  0.10  0.05]\t\t[ 1.77  0.40]\n",
      "[ 0.85  0.36]\t\t[ 0.12  0.07  0.18  0.07  0.23  0.33]\t\t[-1.26  0.92]\n",
      "[ 0.69  0.38]\t\t[ 0.36  0.21  0.12  0.09  0.14  0.07]\t\t[ 2.12  1.03]\n",
      "[ 0.91  0.35]\t\t[ 0.36  0.16  0.14  0.07  0.18  0.10]\t\t[-1.52 -0.48]\n",
      "[ 0.74  0.35]\t\t[ 0.14  0.10  0.18  0.10  0.21  0.26]\t\t[ 1.27 -0.71]\n",
      "[-1.28 -0.29]\t\t[ 0.00  0.07  0.03  0.85  0.01  0.04]\t\t[ 0.44  0.77]\n",
      "  dO obtained from the next layer in backprop\n",
      "[[-0.72 -0.21]\n",
      " [ 0.31  1.48]\n",
      " [ 0.86 -0.16]\n",
      " [-0.02 -1.  ]\n",
      " [-0.02 -0.29]\n",
      " [ 0.32 -0.83]]\n",
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "  dV \t\t=\t\t P.T \t\t\t@\t\t dO\n",
      "[ 0.28 -0.98]\t\t[ 0.92  0.37  1.00  1.00  0.54  0.00]\t\t[-0.72 -0.21]\n",
      "[-0.14 -0.62]\t\t[ 1.00  0.19  0.59  0.43  0.37  0.08]\t\t[ 0.31  1.48]\n",
      "[ 0.18  0.07]\t\t[ 0.37  0.55  0.34  0.38  0.69  0.03]\t\t[ 0.86 -0.16]\n",
      "[ 0.18 -0.98]\t\t[ 0.59  0.21  0.26  0.18  0.39  1.00]\t\t[-0.02 -1.00]\n",
      "[ 0.27  0.13]\t\t[ 0.35  0.68  0.39  0.49  0.79  0.01]\t\t[-0.02 -0.29]\n",
      "[ 0.33  0.81]\t\t[ 0.18  1.00  0.19  0.27  1.00  0.05]\t\t[ 0.32 -0.83]\n",
      "  dP \t\t\t\t=\t\t dO \t\t@\t\t\t V.T\n",
      "[-1.35  0.71 -1.75  1.19 -0.76 -0.48]\t\t[-0.72 -0.21]\t\t[ 1.77 -1.26  2.12 -1.52  1.27  0.44]\n",
      "[ 1.15  0.96  2.18 -1.19 -0.65  1.28]\t\t[ 0.31  1.48]\t\t[ 0.40  0.92  1.03 -0.48 -0.71  0.77]\n",
      "[ 1.45 -1.23  1.65 -1.23  1.20  0.26]\t\t[ 0.86 -0.16]\t\t\t\n",
      "[-0.44 -0.90 -1.08  0.51  0.69 -0.79]\t\t[-0.02 -1.00]\t\t\t\n",
      "[-0.15 -0.24 -0.34  0.17  0.18 -0.23]\t\t[-0.02 -0.29]\t\t\t\n",
      "[ 0.23 -1.17 -0.17 -0.09  0.99 -0.50]\t\t[ 0.32 -0.83]\t\t\t\n",
      "  dS \t\t\t\t=\t\t P \t\t\t\t*\t (dP - np.sum(P * dP))\n",
      "[-0.47  1.55 -0.34  1.19  0.03  0.06]\t\t[ 0.92  1.00  0.37  0.59  0.35  0.18]\t\t[-0.84]\n",
      "[-0.46 -0.28 -0.12 -0.74 -2.06 -1.12]\t\t[ 0.37  0.19  0.55  0.21  0.68  1.00]\t\t[ 2.40]\n",
      "[-0.03 -1.60  0.06 -0.70 -0.11 -0.23]\t\t[ 1.00  0.59  0.34  0.26  0.39  0.19]\t\t[ 1.48]\n",
      "[ 0.57  0.05 -0.02  0.28  0.83  0.06]\t\t[ 1.00  0.43  0.38  0.18  0.49  0.27]\t\t[-1.01]\n",
      "[ 0.15  0.07  0.06  0.23  0.48  0.20]\t\t[ 0.54  0.37  0.69  0.39  0.79  1.00]\t\t[-0.43]\n",
      "[ 0.00 -0.08  0.00  0.11  0.01 -0.01]\t\t[ 0.00  0.08  0.03  1.00  0.01  0.05]\t\t[-0.20]\n",
      "  dQ \t\t=\t\t\t dS \t\t\t@\t K\n",
      "[ 0.73  2.25]\t\t[-0.47  1.55 -0.34  1.19  0.03  0.06]\t\t[-2.03  0.19]\n",
      "[ 3.84  0.47]\t\t[-0.46 -0.28 -0.12 -0.74 -2.06 -1.12]\t\t[-0.66  0.85]\n",
      "[ 0.95 -1.80]\t\t[-0.03 -1.60  0.06 -0.70 -0.11 -0.23]\t\t[-0.79 -0.11]\n",
      "[-2.06  0.08]\t\t[ 0.57  0.05 -0.02  0.28  0.83  0.06]\t\t[ 0.50  0.87]\n",
      "[-0.95 -0.01]\t\t[ 0.15  0.07  0.06  0.23  0.48  0.20]\t\t[-1.20 -0.33]\n",
      "[ 0.09  0.03]\t\t[ 0.00 -0.08  0.00  0.11  0.01 -0.01]\t\t[-0.47 -0.65]\n",
      "  dK \t\t=\t\t\t dS.T \t\t\t@\t Q\n",
      "[-0.20 -0.01]\t\t[-0.47 -0.46 -0.03  0.57  0.15  0.00]\t\t[-0.47  1.09]\n",
      "[ 0.20  0.82]\t\t[ 1.55 -0.28 -1.60  0.05  0.07 -0.08]\t\t[ 0.06 -1.08]\n",
      "[ 0.13 -0.24]\t\t[-0.34 -0.12  0.06 -0.02  0.06  0.00]\t\t[-0.72  0.68]\n",
      "[-0.06  1.60]\t\t[ 1.19 -0.74 -0.70  0.28  0.23  0.11]\t\t[-0.73  0.22]\n",
      "[-0.62  2.06]\t\t[ 0.03 -2.06 -0.11  0.83  0.48  0.01]\t\t[ 0.05 -0.65]\n",
      "[-0.00  1.00]\t\t[ 0.06 -1.12 -0.23  0.06  0.20 -0.01]\t\t[ 2.14  0.63]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=2, suppress=True):\n",
    "\n",
    "    # Forward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    S = Q @ K.T\n",
    "    print(f\"  S \\t\\t\\t=\\t\\t\\t Q \\t\\t@\\t\\t K.T\")\n",
    "    print_matop(S, Q, K.T)\n",
    "\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    P = np.exp(S - m)\n",
    "    l = np.sum(np.exp(S - m), axis=1, keepdims=True)\n",
    "    A = P / l\n",
    "    print(f\"  A \\t\\t\\t=\\t\\t\\t\\t exp(S - m) \\t\\t\\t/\\t\\t sum(exp(S - m))\")\n",
    "    print_matop(A, P, l, separator='\\t\\t\\t')\n",
    "\n",
    "    O = A @ V\n",
    "    print(f\"  O \\t\\t=\\t\\t\\t A \\t\\t\\t@\\t\\t V\")\n",
    "    print_matop(O, A, V)\n",
    "    \n",
    "    dO = np.random.randn(N, d)  # Assume dO comes from the next layer\n",
    "    print(f\"  dO obtained from the next layer in backprop\")\n",
    "    print(dO)\n",
    "\n",
    "    # Backward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    dV = P.T @ dO\n",
    "    print(f\"  dV \\t\\t=\\t\\t P.T \\t\\t\\t@\\t\\t dO\")\n",
    "    print_matop(dV, P.T, dO)\n",
    "\n",
    "    dP = dO @ V.T\n",
    "    print(f\"  dP \\t\\t\\t\\t=\\t\\t dO \\t\\t@\\t\\t\\t V.T\")\n",
    "    print_matop(dP, dO, V.T)\n",
    "\n",
    "    dS = P * (dP - np.sum(P * dP, axis=1, keepdims=True))\n",
    "    print(f\"  dS \\t\\t\\t\\t=\\t\\t P \\t\\t\\t\\t*\\t (dP - np.sum(P * dP))\")\n",
    "    print_matop(dS, P, np.sum(P * dP, axis=1, keepdims=True))\n",
    "\n",
    "    dQ = dS @ K\n",
    "    print(f\"  dQ \\t\\t=\\t\\t\\t dS \\t\\t\\t@\\t K\")\n",
    "    print_matop(dQ, dS, K)\n",
    "\n",
    "    dK = dS.T @ Q\n",
    "    print(f\"  dK \\t\\t=\\t\\t\\t dS.T \\t\\t\\t@\\t Q\")\n",
    "    print_matop(dK, dS.T, Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward Pass Outputs:\n",
      "##################################################\n",
      "\tS \t\t\t\t=\t\t Q \t@\t\t K.T \t\t\t\t*\t\t tau\n",
      "[ 0.82  0.88  0.18  0.50  0.14 -0.34]\t\t[-0.47  1.09]\t\t[-2.03 -0.66 -0.79  0.50 -1.20 -0.47]\t\t 0.71\n",
      "[-0.23 -0.68  0.05 -0.64  0.20  0.48]\t\t[ 0.06 -1.08]\t\t[ 0.19  0.85 -0.11  0.87 -0.33 -0.65]\t\t\t\n",
      "[ 1.11  0.74  0.35  0.16  0.45 -0.07]\t\t[-0.72  0.68]\t\t\t\t\t\t\n",
      "[ 1.07  0.47  0.39 -0.13  0.57  0.15]\t\t[-0.73  0.22]\t\t\t\t\t\t\n",
      "[-0.15 -0.41  0.03 -0.38  0.12  0.29]\t\t[ 0.05 -0.65]\t\t\t\t\t\t\n",
      "[-2.99 -0.62 -1.25  1.15 -1.97 -1.01]\t\t[ 2.14  0.63]\t\t\t\t\t\t\n",
      "\tmasked S\n",
      "[[ 0.82  -inf  -inf  -inf  -inf  -inf]\n",
      " [-0.23 -0.68  -inf  -inf  -inf  -inf]\n",
      " [ 1.11  0.74  0.35  -inf  -inf  -inf]\n",
      " [ 1.07  0.47  0.39 -0.13  -inf  -inf]\n",
      " [-0.15 -0.41  0.03 -0.38  0.12  -inf]\n",
      " [-2.99 -0.62 -1.25  1.15 -1.97 -1.01]]\n",
      "softmax numerator P:\n",
      "\tP \t\t\t\t<=exp\t\t\t (S - m):\n",
      "[ 1.00  0.00  0.00  0.00  0.00  0.00]\t\t[ 0.00  -inf  -inf  -inf  -inf  -inf]\n",
      "[ 1.00  0.64  0.00  0.00  0.00  0.00]\t\t[ 0.00 -0.45  -inf  -inf  -inf  -inf]\n",
      "[ 1.00  0.69  0.46  0.00  0.00  0.00]\t\t[ 0.00 -0.37 -0.77  -inf  -inf  -inf]\n",
      "[ 1.00  0.55  0.51  0.30  0.00  0.00]\t\t[ 0.00 -0.60 -0.68 -1.20  -inf  -inf]\n",
      "[ 0.77  0.59  0.92  0.61  1.00  0.00]\t\t[-0.27 -0.53 -0.09 -0.50  0.00  -inf]\n",
      "[ 0.02  0.17  0.09  1.00  0.04  0.11]\t\t[-4.14 -1.77 -2.41  0.00 -3.12 -2.17]\n",
      "P after drop_out:\n",
      "\tP_dm \t\t\t<=drop_out\t\t\t P \t\t\tby\t\t\t drop_mask:\n",
      "[ 1.11  0.00  0.00  0.00  0.00  0.00]\t\t[ 1.00  0.00  0.00  0.00  0.00  0.00]\t\t[ 1.11  1.11  1.11  0.00  1.11  1.11]\n",
      "[ 1.11  0.71  0.00  0.00  0.00  0.00]\t\t[ 1.00  0.64  0.00  0.00  0.00  0.00]\t\t[ 1.11  1.11  1.11  1.11  1.11  1.11]\n",
      "[ 1.11  0.77  0.52  0.00  0.00  0.00]\t\t[ 1.00  0.69  0.46  0.00  0.00  0.00]\t\t[ 1.11  1.11  1.11  0.00  1.11  1.11]\n",
      "[ 0.00  0.61  0.56  0.33  0.00  0.00]\t\t[ 1.00  0.55  0.51  0.30  0.00  0.00]\t\t[ 0.00  1.11  1.11  1.11  1.11  1.11]\n",
      "[ 0.85  0.65  1.02  0.68  1.11  0.00]\t\t[ 0.77  0.59  0.92  0.61  1.00  0.00]\t\t[ 1.11  1.11  1.11  1.11  1.11  1.11]\n",
      "[ 0.02  0.19  0.00  1.11  0.05  0.00]\t\t[ 0.02  0.17  0.09  1.00  0.04  0.11]\t\t[ 1.11  1.11  0.00  1.11  1.11  0.00]\n",
      "Attention Score A:\n",
      "\t\tA \t\t\t=\t\t\t\t P_dm \t\t\t/\t sum(exp(S_cm - m))\n",
      "[ 0.14  0.00  0.00  0.00  0.00  0.00]\t\t\t[ 1.11  0.00  0.00  0.00  0.00  0.00]\t\t\t[ 7.72]\n",
      "[ 0.13  0.08  0.00  0.00  0.00  0.00]\t\t\t[ 1.11  0.71  0.00  0.00  0.00  0.00]\t\t\t[ 8.62]\n",
      "[ 0.12  0.08  0.06  0.00  0.00  0.00]\t\t\t[ 1.11  0.77  0.52  0.00  0.00  0.00]\t\t\t[ 9.30]\n",
      "[ 0.00  0.06  0.06  0.04  0.00  0.00]\t\t\t[ 0.00  0.61  0.56  0.33  0.00  0.00]\t\t\t[ 9.46]\n",
      "[ 0.07  0.05  0.08  0.06  0.09  0.00]\t\t\t[ 0.85  0.65  1.02  0.68  1.11  0.00]\t\t\t[12.01]\n",
      "[ 0.00  0.02  0.00  0.14  0.01  0.00]\t\t\t[ 0.02  0.19  0.00  1.11  0.05  0.00]\t\t\t[ 8.18]\n",
      "output matrix O:\n",
      "\tO \t=\t\t\t A \t\t\t@\t\t V\n",
      "[ 0.25  0.06]\t\t[ 0.14  0.00  0.00  0.00  0.00  0.00]\t\t[ 1.77  0.40]\n",
      "[ 0.12  0.13]\t\t[ 0.13  0.08  0.00  0.00  0.00  0.00]\t\t[-1.26  0.92]\n",
      "[ 0.22  0.18]\t\t[ 0.12  0.08  0.06  0.00  0.00  0.00]\t\t[ 2.12  1.03]\n",
      "[-0.01  0.10]\t\t[ 0.00  0.06  0.06  0.04  0.00  0.00]\t\t[-1.52 -0.48]\n",
      "[ 0.27  0.07]\t\t[ 0.07  0.05  0.08  0.06  0.09  0.00]\t\t[ 1.27 -0.71]\n",
      "[-0.22 -0.05]\t\t[ 0.00  0.02  0.00  0.14  0.01  0.00]\t\t[ 0.44  0.77]\n",
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "\tdO obtained from the next layer in backprop\n",
      "[[ 2.56 -0.1 ]\n",
      " [ 1.15 -0.7 ]\n",
      " [-0.03  1.77]\n",
      " [-0.63  1.81]\n",
      " [ 0.71 -0.56]\n",
      " [ 0.63  0.97]]\n",
      "\tdV \t\t=\t\t P.T \t\t\t@\t\t dO\n",
      "[ 4.70  0.62]\t\t[ 1.00  1.00  1.00  1.00  0.77  0.02]\t\t[ 2.56 -0.10]\n",
      "[ 0.99  1.78]\t\t[ 0.00  0.64  0.69  0.55  0.59  0.17]\t\t[ 1.15 -0.70]\n",
      "[ 0.35  1.36]\t\t[ 0.00  0.00  0.46  0.51  0.92  0.09]\t\t[-0.03  1.77]\n",
      "[ 0.97  1.31]\t\t[ 0.00  0.00  0.00  0.30  0.61  1.00]\t\t[-0.63  1.81]\n",
      "[ 0.82 -0.58]\t\t[ 0.00  0.00  0.00  0.00  1.00  0.04]\t\t[ 0.71 -0.56]\n",
      "[ 0.00  0.00]\t\t[ 0.00  0.00  0.00  0.00  0.00  0.11]\t\t[ 0.63  0.97]\n",
      "\tdP_dm \t\t\t\t=\t\t dO \t@\t\t\t V.T\n",
      "[ 4.48 -3.32  5.33 -3.84  3.31  1.06]\t\t[ 2.56 -0.10]\t\t[ 1.77 -1.26  2.12 -1.52  1.27  0.44]\n",
      "[ 1.74 -2.09  1.71 -1.41  1.95 -0.03]\t\t[ 1.15 -0.70]\t\t[ 0.40  0.92  1.03 -0.48 -0.71  0.77]\n",
      "[ 0.66  1.67  1.75 -0.80 -1.30  1.36]\t\t[-0.03  1.77]\t\t\t\n",
      "[-0.37  2.45  0.54  0.07 -2.08  1.13]\t\t[-0.63  1.81]\t\t\t\n",
      "[ 1.02 -1.41  0.92 -0.80  1.29 -0.12]\t\t[ 0.71 -0.56]\t\t\t\n",
      "[ 1.51  0.10  2.35 -1.43  0.11  1.03]\t\t[ 0.63  0.97]\t\t\t\n",
      "\tdS \t\t\t\t=\t\t P \t\t\t*\t (dP - np.sum(P * dP))\n",
      "[ 0.00 -0.00  0.00 -0.00 -0.00 -0.00]\t\t[ 1.00  0.00  0.00  0.00  0.00  0.00]\t\t[ 4.98]\n",
      "[ 1.49 -1.78  0.00 -0.00  0.00 -0.00]\t\t[ 1.00  0.64  0.00  0.00  0.00  0.00]\t\t[ 0.45]\n",
      "[-2.19 -0.73 -0.45 -0.00 -0.00 -0.00]\t\t[ 1.00  0.69  0.46  0.00  0.00  0.00]\t\t[ 2.91]\n",
      "[-1.82  0.50 -0.62 -0.52 -0.00 -0.00]\t\t[ 1.00  0.55  0.51  0.30  0.00  0.00]\t\t[ 1.82]\n",
      "[-0.50 -1.97 -0.69 -1.62 -0.34 -0.00]\t\t[ 0.77  0.59  0.92  0.61  1.00  0.00]\t\t[ 1.78]\n",
      "[ 0.05  0.28  0.14 -0.05  0.07  0.18]\t\t[ 0.02  0.17  0.09  1.00  0.04  0.11]\t\t[-1.54]\n",
      "\tdQ \t=\t\t\t dS \t\t\t@\t\t K\n",
      "[ 0.00  0.00]\t\t[ 0.00 -0.00  0.00 -0.00 -0.00 -0.00]\t\t[-2.03  0.19]\n",
      "[-1.84 -1.24]\t\t[ 1.49 -1.78  0.00 -0.00  0.00 -0.00]\t\t[-0.66  0.85]\n",
      "[ 5.27 -0.98]\t\t[-2.19 -0.73 -0.45 -0.00 -0.00 -0.00]\t\t[-0.79 -0.11]\n",
      "[ 3.59 -0.30]\t\t[-1.82  0.50 -0.62 -0.52 -0.00 -0.00]\t\t[ 0.50  0.87]\n",
      "[ 2.45 -2.98]\t\t[-0.50 -1.97 -0.69 -1.62 -0.34 -0.00]\t\t[-1.20 -0.33]\n",
      "[-0.60  0.05]\t\t[ 0.05  0.28  0.14 -0.05  0.07  0.18]\t\t[-0.47 -0.65]\n",
      "\tdK \t=\t\t\t dS.T \t\t\t@\t\t Q\n",
      "[ 3.08 -3.13]\t\t[ 0.00  1.49 -2.19 -1.82 -0.50  0.05]\t\t[-0.47  1.09]\n",
      "[ 0.56  2.99]\t\t[-0.00 -1.78 -0.73  0.50 -1.97  0.28]\t\t[ 0.06 -1.08]\n",
      "[ 1.04  0.10]\t\t[ 0.00  0.00 -0.45 -0.62 -0.69  0.14]\t\t[-0.72  0.68]\n",
      "[ 0.20  0.91]\t\t[-0.00 -0.00 -0.00 -0.52 -1.62 -0.05]\t\t[-0.73  0.22]\n",
      "[ 0.14  0.27]\t\t[-0.00  0.00 -0.00 -0.00 -0.34  0.07]\t\t[ 0.05 -0.65]\n",
      "[ 0.38  0.11]\t\t[-0.00 -0.00 -0.00 -0.00 -0.00  0.18]\t\t[ 2.14  0.63]\n"
     ]
    }
   ],
   "source": [
    "tau = 1 / np.sqrt(d)\n",
    "p_drop = 0.1\n",
    "\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "\n",
    "    # Forward pass\n",
    "    print(\"\\nForward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    S = tau * Q @ K.T\n",
    "    print(f\"\\tS \\t\\t\\t\\t=\\t\\t Q \\t@\\t\\t K.T \\t\\t\\t\\t*\\t tau\")\n",
    "    print_matop(S, Q, K.T, np.array([tau]))\n",
    "\n",
    "    # Apply causal mask directly to S\n",
    "    S_cm = np.where(np.tril(np.ones_like(S), k=0) == 1, S, -np.inf)\n",
    "    print(f\"\\tmasked S\")\n",
    "    print(S_cm)\n",
    "\n",
    "    m = np.max(S_cm, axis=1, keepdims=True)\n",
    "    S_ = S_cm - m\n",
    "    P = np.exp(S_)\n",
    "    print(f\"softmax numerator P:\")\n",
    "    print(f\"\\tP \\t\\t\\t\\t<=exp\\t\\t\\t (S - m):\")\n",
    "    print_matop(P, S_, separator='\\t\\t')\n",
    "\n",
    "    l = np.sum(np.exp(P), axis=1, keepdims=True)\n",
    "\n",
    "    # Apply dropout to P\n",
    "    drop_mask = ((np.random.rand(*P.shape) > p_drop) / (1 - p_drop))\n",
    "    P_dm = P * drop_mask \n",
    "    print(f\"P after drop_out:\")\n",
    "    print(f\"\\tP_dm \\t\\t\\t<=drop_out\\t\\t\\t P \\t\\t\\tby\\t\\t\\t drop_mask:\")\n",
    "    print_matop(P_dm, P, drop_mask, separator='\\t\\t')\n",
    "\n",
    "    A = P_dm / l\n",
    "    print(f\"Attention Score A:\")\n",
    "    print(f\"\\t\\tA \\t\\t\\t=\\t\\t\\t\\t P_dm \\t\\t\\t/\\t sum(exp(S_cm - m))\")\n",
    "    print_matop(A, P_dm, l, separator='\\t\\t\\t')\n",
    "\n",
    "    O = A @ V\n",
    "    print(f\"output matrix O:\")\n",
    "    print(f\"\\tO \\t=\\t\\t\\t A \\t\\t\\t@\\t\\t V\")\n",
    "    print_matop(O, A, V)\n",
    "\n",
    "    # Backward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")    \n",
    "    print(\"#\" * 50)\n",
    "    dO = np.random.randn(N, d)  # Assume dO comes from the next layer\n",
    "    print(f\"\\tdO obtained from the next layer in backprop\")\n",
    "    print(dO)\n",
    "\n",
    "    dV = P_dm.T @ dO\n",
    "    print(f\"\\tdV \\t\\t=\\t\\t P.T \\t\\t\\t@\\t\\t dO\")\n",
    "    print_matop(dV, P.T, dO)\n",
    "\n",
    "    dP_dm = dO @ V.T\n",
    "    print(f\"\\tdP_dm \\t\\t\\t\\t=\\t\\t dO \\t@\\t\\t\\t V.T\")\n",
    "    print_matop(dP_dm, dO, V.T)\n",
    "\n",
    "    dP = dP_dm * drop_mask\n",
    "\n",
    "    dS = P * (dP - np.sum(P * dP, axis=1, keepdims=True))\n",
    "    print(f\"\\tdS \\t\\t\\t\\t=\\t\\t P \\t\\t\\t*\\t (dP - np.sum(P * dP))\")\n",
    "    print_matop(dS, P, np.sum(P * dP, axis=1, keepdims=True))\n",
    "\n",
    "    dQ = dS @ K * tau\n",
    "    print(f\"\\tdQ \\t=\\t\\t\\t dS \\t\\t\\t@\\t\\t K\\t*\\t tau\")\n",
    "    print_matop(dQ, dS, K, np.array([tau]))\n",
    "\n",
    "    dK = dS.T @ Q * tau\n",
    "    print(f\"\\tdK \\t=\\t\\t\\t dS.T \\t\\t\\t@\\t\\t Q\\t*\\t tau\")\n",
    "    print_matop(dK, dS.T, Q, np.array([tau]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass Output (O):\n",
      "[[ 0.51934651        -inf        -inf        -inf]\n",
      " [ 0.69014399 -0.40122047        -inf        -inf]\n",
      " [ 0.0976761  -0.77300978  0.02451017        -inf]\n",
      " [ 1.45114361  0.95927083  2.15318246 -0.76734756]]\n",
      "\n",
      "Backward Pass Output (dI):\n",
      "[[ 0.87232064  0.          0.         -0.        ]\n",
      " [-0.83972184 -0.59939265 -0.         -0.        ]\n",
      " [-0.75913266  0.15039379  0.34175598  0.        ]\n",
      " [ 0.95042384 -0.57690366 -0.89841467  0.49191917]]\n"
     ]
    }
   ],
   "source": [
    "def causal_mask(I):\n",
    "    mask = np.tril(np.ones_like(I), k=0)  # Lower triangular matrix with ones\n",
    "    O = np.where(mask == 0, -np.inf, I)   # Apply the mask\n",
    "    return O, mask\n",
    "\n",
    "def d_causal_mask(dO, mask):\n",
    "    dI = dO * mask  # Propagate gradient only through unmasked elements (mask == 1)\n",
    "    return dI\n",
    "\n",
    "I = np.random.randn(4, 4)\n",
    "\n",
    "# Forward pass\n",
    "O, mask = causal_mask(I)\n",
    "print(\"Forward Pass Output (O):\")\n",
    "print(O)\n",
    "\n",
    "# Assume some gradient coming from the next layer\n",
    "dO = np.random.randn(*O.shape)\n",
    "\n",
    "# Backward pass\n",
    "dI = d_causal_mask(dO, mask)\n",
    "print(\"\\nBackward Pass Output (dI):\")\n",
    "print(dI)\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\n",
      "[[-0.02090159  0.11732738  1.2776649  -0.59157139]\n",
      " [ 0.54709738 -0.20219265 -0.2176812   1.09877685]\n",
      " [ 0.82541635  0.81350964  1.30547881  0.02100384]\n",
      " [ 0.68195297 -0.31026676  0.32416635 -0.13014305]]\n",
      "O:\n",
      "[[0.15680308 0.18004733 0.57453284 0.08861676]\n",
      " [0.27216028 0.12865072 0.12667346 0.47251554]\n",
      " [0.2468106  0.24388932 0.3988892  0.11041088]\n",
      " [0.39778803 0.14748103 0.27814225 0.17658869]]\n",
      "dO:\n",
      "[[ 0.09699596  0.59515703 -0.81822068  2.09238728]\n",
      " [-1.00601738 -1.21418861  1.15811087  0.79166269]\n",
      " [ 0.62411982  0.62834551 -0.01224677 -0.89725437]\n",
      " [ 0.07580456 -0.67716171  0.97511973 -0.14705738]]\n",
      "dI:\n",
      "[[ 0.04065972  0.13637962 -0.37684316  0.19980382]\n",
      " [-0.29850213 -0.16788394  0.13520368  0.3311824 ]\n",
      " [ 0.1038543   0.10365567 -0.085993   -0.12151698]\n",
      " [-0.03967315 -0.12575717  0.22239719 -0.05696687]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(I: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    I: (n, m)\n",
    "    O: (n, m)\n",
    "    \"\"\"\n",
    "    exp_I = np.exp(I - np.max(I, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    O = exp_I / np.sum(exp_I, axis=1, keepdims=True)\n",
    "    return O\n",
    "\n",
    "def d_softmax(dO: np.ndarray, O: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    dO: (n, m)\n",
    "    O: (n, m)\n",
    "    dI: (n, m)\n",
    "    \"\"\"\n",
    "    n, m = O.shape\n",
    "    dI = np.zeros_like(dO)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Reshape O[i] to a column vector of shape (m, 1)\n",
    "        O_i = O[i].reshape(-1, 1)\n",
    "        # Compute the Jacobian matrix of the softmax function for this row, shape (m, m)\n",
    "        jacobian_i = np.diagflat(O_i) - np.dot(O_i, O_i.T)\n",
    "        # Compute the gradient for this row, shape (m,)\n",
    "        dI[i] = np.dot(jacobian_i, dO[i])\n",
    "    return dI\n",
    "\n",
    "# Example input matrix I of shape (4, 4)\n",
    "I = np.random.randn(4, 4)\n",
    "print(\"I:\")\n",
    "print(I)\n",
    "# Forward pass\n",
    "O = softmax(I)\n",
    "print(\"O:\")\n",
    "print(O)\n",
    "\n",
    "# Assume some gradient coming from the next layer, of shape (4, 4)\n",
    "dO = np.random.randn(*O.shape)\n",
    "print(\"dO:\")\n",
    "print(dO)\n",
    "\n",
    "# Backward pass\n",
    "dI = d_softmax(dO, O)\n",
    "print(\"dI:\")\n",
    "print(dI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISG_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matop(*matrices, separator=\"\\t\\t\", width=5):\n",
    "    \"\"\"\n",
    "    Prints multiple matrices side by side, ensuring proper alignment.\n",
    "    \n",
    "    Args:\n",
    "    - matrices: The matrices to print side by side.\n",
    "    - separator: The string used to separate the matrices in the output.\n",
    "    - width: The fixed width for each element in the matrix to ensure alignment.\n",
    "    \"\"\"\n",
    "    # Find the maximum number of rows across all matrices\n",
    "    max_rows = max(len(matrix) for matrix in matrices)\n",
    "    \n",
    "    # Iterate over the rows by index up to the max number of rows\n",
    "    for i in range(max_rows):\n",
    "        formatted_rows = []\n",
    "        for matrix in matrices:\n",
    "            # Check if the matrix has a row at index i\n",
    "            if i < len(matrix):\n",
    "                row = matrix[i]\n",
    "                # If row is iterable (like a list or numpy array), format each element\n",
    "                if hasattr(row, '__iter__'):\n",
    "                    formatted_rows.append(\"[\" + \" \".join(f\"{val:{width}.2f}\" for val in row) + \"]\")\n",
    "                else:\n",
    "                    # If it's a single scalar value, format it directly\n",
    "                    formatted_rows.append(f\"  {row:{width}.2f}\")\n",
    "            else:\n",
    "                # Add an empty value if the matrix does not have enough rows\n",
    "                formatted_rows.append('\\t' * (width // 4))\n",
    "        \n",
    "        # Join the formatted rows with the specified separator and print them\n",
    "        print(separator.join(formatted_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bc: 3\n",
      "Br: 2\n",
      "Q \t\t K \t\t V\n",
      "[-0.47  1.09] \t [-2.03  0.19] \t [1.77 0.4 ]\n",
      "[ 0.06 -1.08] \t [-0.66  0.85] \t [-1.26  0.92]\n",
      "[-0.72  0.68] \t [-0.79 -0.11] \t [2.12 1.03]\n",
      "[-0.73  0.22] \t [0.5  0.87] \t [-1.52 -0.48]\n",
      "[ 0.05 -0.65] \t [-1.2  -0.33] \t [ 1.27 -0.71]\n",
      "[2.14 0.63] \t [-0.47 -0.65] \t [0.44 0.77]\n"
     ]
    }
   ],
   "source": [
    "# SRAM memory size\n",
    "M = 20\n",
    "# simplified head size = hidden size\n",
    "d = 2\n",
    "# Sequence length\n",
    "N = 6\n",
    "\n",
    "# set block size for outer loop\n",
    "Bc = math.ceil(M / (4 * d))\n",
    "print(f\"Bc: {Bc}\")\n",
    "# set block size for inner loop\n",
    "Br = min(Bc, d)\n",
    "print(f\"Br: {Br}\")\n",
    "\n",
    "# Example dimensions for Q, K, V matrices\n",
    "Q = np.random.randn(N, d)\n",
    "K = np.random.randn(N, d)\n",
    "V = np.random.randn(N, d)\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Q \\t\\t K \\t\\t V\")\n",
    "    for q, k, v in zip(Q, K, V):\n",
    "        print(f\"{q} \\t {k} \\t {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Forward Pass\n",
    "- direct attention computation using numpy/scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output O:\n",
      "[[ 0.23  0.37]\n",
      " [ 0.85  0.36]\n",
      " [ 0.69  0.38]\n",
      " [ 0.91  0.35]\n",
      " [ 0.74  0.35]\n",
      " [-1.28 -0.29]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax  # Optional, can use numpy's method or scipy's\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    # Step 1: Calculate the dot product of Q and K.T (scores)\n",
    "    S = np.dot(Q, K.T)  # (N, d) @ (d, N) -> (N, N)\n",
    "    \n",
    "    # Step 2: Apply the softmax to the scores for each row\n",
    "    P = softmax(S, axis=1)  # Softmax along the rows\n",
    "    \n",
    "    # Step 3: Multiply the attention weights with the value matrix V\n",
    "    O = np.dot(P, V)  # (N, N) @ (N, d) -> (N, d)\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Calculate the attention output\n",
    "_O = attention(Q, K, V)\n",
    "\n",
    "# Print the result\n",
    "print(\"Attention Output O:\")\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(_O)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward + Backward Pass\n",
    "\n",
    "### Simplified Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "  S \t\t\t=\t\t\t Q \t\t@\t\t K.T\n",
      "[ 1.16  1.24  0.25  0.70  0.20 -0.49]\t\t[-0.47  1.09]\t\t[-2.03 -0.66 -0.79  0.50 -1.20 -0.47]\n",
      "[-0.33 -0.96  0.07 -0.90  0.28  0.67]\t\t[ 0.06 -1.08]\t\t[ 0.19  0.85 -0.11  0.87 -0.33 -0.65]\n",
      "[ 1.58  1.05  0.49  0.23  0.63 -0.10]\t\t[-0.72  0.68]\t\t\t\n",
      "[ 1.52  0.67  0.55 -0.18  0.80  0.21]\t\t[-0.73  0.22]\t\t\t\n",
      "[-0.21 -0.59  0.04 -0.54  0.16  0.40]\t\t[ 0.05 -0.65]\t\t\t\n",
      "[-4.22 -0.88 -1.77  1.63 -2.79 -1.43]\t\t[ 2.14  0.63]\t\t\t\n",
      "  A \t\t\t=\t\t\t\t exp(S - m) \t\t\t/\t\t sum(exp(S - m))\n",
      "[ 0.27  0.29  0.11  0.17  0.10  0.05]\t\t\t[ 0.92  1.00  0.37  0.59  0.35  0.18]\t\t\t[ 3.41]\n",
      "[ 0.12  0.07  0.18  0.07  0.23  0.33]\t\t\t[ 0.37  0.19  0.55  0.21  0.68  1.00]\t\t\t[ 2.99]\n",
      "[ 0.36  0.21  0.12  0.09  0.14  0.07]\t\t\t[ 1.00  0.59  0.34  0.26  0.39  0.19]\t\t\t[ 2.77]\n",
      "[ 0.36  0.16  0.14  0.07  0.18  0.10]\t\t\t[ 1.00  0.43  0.38  0.18  0.49  0.27]\t\t\t[ 2.75]\n",
      "[ 0.14  0.10  0.18  0.10  0.21  0.26]\t\t\t[ 0.54  0.37  0.69  0.39  0.79  1.00]\t\t\t[ 3.78]\n",
      "[ 0.00  0.07  0.03  0.85  0.01  0.04]\t\t\t[ 0.00  0.08  0.03  1.00  0.01  0.05]\t\t\t[ 1.18]\n",
      "  O \t\t=\t\t\t A \t\t\t@\t\t V\n",
      "[ 0.23  0.37]\t\t[ 0.27  0.29  0.11  0.17  0.10  0.05]\t\t[ 1.77  0.40]\n",
      "[ 0.85  0.36]\t\t[ 0.12  0.07  0.18  0.07  0.23  0.33]\t\t[-1.26  0.92]\n",
      "[ 0.69  0.38]\t\t[ 0.36  0.21  0.12  0.09  0.14  0.07]\t\t[ 2.12  1.03]\n",
      "[ 0.91  0.35]\t\t[ 0.36  0.16  0.14  0.07  0.18  0.10]\t\t[-1.52 -0.48]\n",
      "[ 0.74  0.35]\t\t[ 0.14  0.10  0.18  0.10  0.21  0.26]\t\t[ 1.27 -0.71]\n",
      "[-1.28 -0.29]\t\t[ 0.00  0.07  0.03  0.85  0.01  0.04]\t\t[ 0.44  0.77]\n",
      "  dO obtained from the next layer in backprop\n",
      "[[-0.72 -0.21]\n",
      " [ 0.31  1.48]\n",
      " [ 0.86 -0.16]\n",
      " [-0.02 -1.  ]\n",
      " [-0.02 -0.29]\n",
      " [ 0.32 -0.83]]\n",
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "  dV \t\t=\t\t P.T \t\t\t@\t\t dO\n",
      "[ 0.28 -0.98]\t\t[ 0.92  0.37  1.00  1.00  0.54  0.00]\t\t[-0.72 -0.21]\n",
      "[-0.14 -0.62]\t\t[ 1.00  0.19  0.59  0.43  0.37  0.08]\t\t[ 0.31  1.48]\n",
      "[ 0.18  0.07]\t\t[ 0.37  0.55  0.34  0.38  0.69  0.03]\t\t[ 0.86 -0.16]\n",
      "[ 0.18 -0.98]\t\t[ 0.59  0.21  0.26  0.18  0.39  1.00]\t\t[-0.02 -1.00]\n",
      "[ 0.27  0.13]\t\t[ 0.35  0.68  0.39  0.49  0.79  0.01]\t\t[-0.02 -0.29]\n",
      "[ 0.33  0.81]\t\t[ 0.18  1.00  0.19  0.27  1.00  0.05]\t\t[ 0.32 -0.83]\n",
      "  dP \t\t\t\t=\t\t dO \t\t@\t\t\t V.T\n",
      "[-1.35  0.71 -1.75  1.19 -0.76 -0.48]\t\t[-0.72 -0.21]\t\t[ 1.77 -1.26  2.12 -1.52  1.27  0.44]\n",
      "[ 1.15  0.96  2.18 -1.19 -0.65  1.28]\t\t[ 0.31  1.48]\t\t[ 0.40  0.92  1.03 -0.48 -0.71  0.77]\n",
      "[ 1.45 -1.23  1.65 -1.23  1.20  0.26]\t\t[ 0.86 -0.16]\t\t\t\n",
      "[-0.44 -0.90 -1.08  0.51  0.69 -0.79]\t\t[-0.02 -1.00]\t\t\t\n",
      "[-0.15 -0.24 -0.34  0.17  0.18 -0.23]\t\t[-0.02 -0.29]\t\t\t\n",
      "[ 0.23 -1.17 -0.17 -0.09  0.99 -0.50]\t\t[ 0.32 -0.83]\t\t\t\n",
      "  dS \t\t\t\t=\t\t P \t\t\t\t*\t (dP - np.sum(P * dP))\n",
      "[-0.47  1.55 -0.34  1.19  0.03  0.06]\t\t[ 0.92  1.00  0.37  0.59  0.35  0.18]\t\t[-0.84]\n",
      "[-0.46 -0.28 -0.12 -0.74 -2.06 -1.12]\t\t[ 0.37  0.19  0.55  0.21  0.68  1.00]\t\t[ 2.40]\n",
      "[-0.03 -1.60  0.06 -0.70 -0.11 -0.23]\t\t[ 1.00  0.59  0.34  0.26  0.39  0.19]\t\t[ 1.48]\n",
      "[ 0.57  0.05 -0.02  0.28  0.83  0.06]\t\t[ 1.00  0.43  0.38  0.18  0.49  0.27]\t\t[-1.01]\n",
      "[ 0.15  0.07  0.06  0.23  0.48  0.20]\t\t[ 0.54  0.37  0.69  0.39  0.79  1.00]\t\t[-0.43]\n",
      "[ 0.00 -0.08  0.00  0.11  0.01 -0.01]\t\t[ 0.00  0.08  0.03  1.00  0.01  0.05]\t\t[-0.20]\n",
      "  dQ \t\t=\t\t\t dS \t\t\t@\t K\n",
      "[ 0.73  2.25]\t\t[-0.47  1.55 -0.34  1.19  0.03  0.06]\t\t[-2.03  0.19]\n",
      "[ 3.84  0.47]\t\t[-0.46 -0.28 -0.12 -0.74 -2.06 -1.12]\t\t[-0.66  0.85]\n",
      "[ 0.95 -1.80]\t\t[-0.03 -1.60  0.06 -0.70 -0.11 -0.23]\t\t[-0.79 -0.11]\n",
      "[-2.06  0.08]\t\t[ 0.57  0.05 -0.02  0.28  0.83  0.06]\t\t[ 0.50  0.87]\n",
      "[-0.95 -0.01]\t\t[ 0.15  0.07  0.06  0.23  0.48  0.20]\t\t[-1.20 -0.33]\n",
      "[ 0.09  0.03]\t\t[ 0.00 -0.08  0.00  0.11  0.01 -0.01]\t\t[-0.47 -0.65]\n",
      "  dK \t\t=\t\t\t dS.T \t\t\t@\t Q\n",
      "[-0.20 -0.01]\t\t[-0.47 -0.46 -0.03  0.57  0.15  0.00]\t\t[-0.47  1.09]\n",
      "[ 0.20  0.82]\t\t[ 1.55 -0.28 -1.60  0.05  0.07 -0.08]\t\t[ 0.06 -1.08]\n",
      "[ 0.13 -0.24]\t\t[-0.34 -0.12  0.06 -0.02  0.06  0.00]\t\t[-0.72  0.68]\n",
      "[-0.06  1.60]\t\t[ 1.19 -0.74 -0.70  0.28  0.23  0.11]\t\t[-0.73  0.22]\n",
      "[-0.62  2.06]\t\t[ 0.03 -2.06 -0.11  0.83  0.48  0.01]\t\t[ 0.05 -0.65]\n",
      "[-0.00  1.00]\t\t[ 0.06 -1.12 -0.23  0.06  0.20 -0.01]\t\t[ 2.14  0.63]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=2, suppress=True):\n",
    "\n",
    "    # Forward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    S = Q @ K.T\n",
    "    print(f\"  S \\t\\t\\t=\\t\\t\\t Q \\t\\t@\\t\\t K.T\")\n",
    "    print_matop(S, Q, K.T)\n",
    "\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    P = np.exp(S - m)\n",
    "    l = np.sum(np.exp(S - m), axis=1, keepdims=True)\n",
    "    A = P / l\n",
    "    print(f\"  A \\t\\t\\t=\\t\\t\\t\\t exp(S - m) \\t\\t\\t/\\t\\t sum(exp(S - m))\")\n",
    "    print_matop(A, P, l, separator='\\t\\t\\t')\n",
    "\n",
    "    O = A @ V\n",
    "    print(f\"  O \\t\\t=\\t\\t\\t A \\t\\t\\t@\\t\\t V\")\n",
    "    print_matop(O, A, V)\n",
    "    \n",
    "    dO = np.random.randn(N, d)  # Assume dO comes from the next layer\n",
    "    print(f\"  dO obtained from the next layer in backprop\")\n",
    "    print(dO)\n",
    "\n",
    "    # Backward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    dV = P.T @ dO\n",
    "    print(f\"  dV \\t\\t=\\t\\t P.T \\t\\t\\t@\\t\\t dO\")\n",
    "    print_matop(dV, P.T, dO)\n",
    "\n",
    "    dP = dO @ V.T\n",
    "    print(f\"  dP \\t\\t\\t\\t=\\t\\t dO \\t\\t@\\t\\t\\t V.T\")\n",
    "    print_matop(dP, dO, V.T)\n",
    "\n",
    "    dS = P * (dP - np.sum(P * dP, axis=1, keepdims=True))\n",
    "    print(f\"  dS \\t\\t\\t\\t=\\t\\t P \\t\\t\\t\\t*\\t (dP - np.sum(P * dP))\")\n",
    "    print_matop(dS, P, np.sum(P * dP, axis=1, keepdims=True))\n",
    "\n",
    "    dQ = dS @ K\n",
    "    print(f\"  dQ \\t\\t=\\t\\t\\t dS \\t\\t\\t@\\t K\")\n",
    "    print_matop(dQ, dS, K)\n",
    "\n",
    "    dK = dS.T @ Q\n",
    "    print(f\"  dK \\t\\t=\\t\\t\\t dS.T \\t\\t\\t@\\t Q\")\n",
    "    print_matop(dK, dS.T, Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward Pass Outputs:\n",
      "##################################################\n",
      "\tS \t\t\t\t=\t\t Q \t@\t\t K.T \t\t\t\t*\t tau\n",
      "[ 0.82  0.88  0.18  0.50  0.14 -0.34]\t\t[-0.47  1.09]\t\t[-2.03 -0.66 -0.79  0.50 -1.20 -0.47]\t\t 0.71\n",
      "[-0.23 -0.68  0.05 -0.64  0.20  0.48]\t\t[ 0.06 -1.08]\t\t[ 0.19  0.85 -0.11  0.87 -0.33 -0.65]\t\t\t\n",
      "[ 1.11  0.74  0.35  0.16  0.45 -0.07]\t\t[-0.72  0.68]\t\t\t\t\t\t\n",
      "[ 1.07  0.47  0.39 -0.13  0.57  0.15]\t\t[-0.73  0.22]\t\t\t\t\t\t\n",
      "[-0.15 -0.41  0.03 -0.38  0.12  0.29]\t\t[ 0.05 -0.65]\t\t\t\t\t\t\n",
      "[-2.99 -0.62 -1.25  1.15 -1.97 -1.01]\t\t[ 2.14  0.63]\t\t\t\t\t\t\n",
      "\tmasked S\n",
      "[[ 0.82  -inf  -inf  -inf  -inf  -inf]\n",
      " [-0.23 -0.68  -inf  -inf  -inf  -inf]\n",
      " [ 1.11  0.74  0.35  -inf  -inf  -inf]\n",
      " [ 1.07  0.47  0.39 -0.13  -inf  -inf]\n",
      " [-0.15 -0.41  0.03 -0.38  0.12  -inf]\n",
      " [-2.99 -0.62 -1.25  1.15 -1.97 -1.01]]\n",
      "softmax numerator P:\n",
      "\tP \t\t\t\t<=exp\t\t\t (S - m):\n",
      "[ 1.00  0.00  0.00  0.00  0.00  0.00]\t\t[ 0.00  -inf  -inf  -inf  -inf  -inf]\n",
      "[ 1.00  0.64  0.00  0.00  0.00  0.00]\t\t[ 0.00 -0.45  -inf  -inf  -inf  -inf]\n",
      "[ 1.00  0.69  0.46  0.00  0.00  0.00]\t\t[ 0.00 -0.37 -0.77  -inf  -inf  -inf]\n",
      "[ 1.00  0.55  0.51  0.30  0.00  0.00]\t\t[ 0.00 -0.60 -0.68 -1.20  -inf  -inf]\n",
      "[ 0.77  0.59  0.92  0.61  1.00  0.00]\t\t[-0.27 -0.53 -0.09 -0.50  0.00  -inf]\n",
      "[ 0.02  0.17  0.09  1.00  0.04  0.11]\t\t[-4.14 -1.77 -2.41  0.00 -3.12 -2.17]\n",
      "P after drop_out:\n",
      "\tP_dm \t\t\t<=drop_out\t\t\t P \t\t\tby\t\t\t drop_mask:\n",
      "[ 1.11  0.00  0.00  0.00  0.00  0.00]\t\t[ 1.00  0.00  0.00  0.00  0.00  0.00]\t\t[ 1.11  1.11  1.11  0.00  1.11  1.11]\n",
      "[ 1.11  0.71  0.00  0.00  0.00  0.00]\t\t[ 1.00  0.64  0.00  0.00  0.00  0.00]\t\t[ 1.11  1.11  1.11  1.11  1.11  1.11]\n",
      "[ 1.11  0.77  0.52  0.00  0.00  0.00]\t\t[ 1.00  0.69  0.46  0.00  0.00  0.00]\t\t[ 1.11  1.11  1.11  1.11  1.11  1.11]\n",
      "[ 1.11  0.61  0.56  0.00  0.00  0.00]\t\t[ 1.00  0.55  0.51  0.30  0.00  0.00]\t\t[ 1.11  1.11  1.11  0.00  1.11  1.11]\n",
      "[ 0.85  0.65  1.02  0.00  1.11  0.00]\t\t[ 0.77  0.59  0.92  0.61  1.00  0.00]\t\t[ 1.11  1.11  1.11  0.00  1.11  1.11]\n",
      "[ 0.02  0.19  0.10  1.11  0.05  0.13]\t\t[ 0.02  0.17  0.09  1.00  0.04  0.11]\t\t[ 1.11  1.11  1.11  1.11  1.11  1.11]\n",
      "Attention Score A:\n",
      "\t\tA \t\t\t=\t\t\t\t P_dm \t\t\t/\t sum(exp(S_cm - m))\n",
      "[ 0.14  0.00  0.00  0.00  0.00  0.00]\t\t\t[ 1.11  0.00  0.00  0.00  0.00  0.00]\t\t\t[ 7.72]\n",
      "[ 0.13  0.08  0.00  0.00  0.00  0.00]\t\t\t[ 1.11  0.71  0.00  0.00  0.00  0.00]\t\t\t[ 8.62]\n",
      "[ 0.12  0.08  0.06  0.00  0.00  0.00]\t\t\t[ 1.11  0.77  0.52  0.00  0.00  0.00]\t\t\t[ 9.30]\n",
      "[ 0.12  0.06  0.06  0.00  0.00  0.00]\t\t\t[ 1.11  0.61  0.56  0.00  0.00  0.00]\t\t\t[ 9.46]\n",
      "[ 0.07  0.05  0.08  0.00  0.09  0.00]\t\t\t[ 0.85  0.65  1.02  0.00  1.11  0.00]\t\t\t[12.01]\n",
      "[ 0.00  0.02  0.01  0.14  0.01  0.02]\t\t\t[ 0.02  0.19  0.10  1.11  0.05  0.13]\t\t\t[ 8.18]\n",
      "output matrix O:\n",
      "\tO \t=\t\t\t A \t\t\t@\t\t V\n",
      "[ 0.25  0.06]\t\t[ 0.14  0.00  0.00  0.00  0.00  0.00]\t\t[ 1.77  0.40]\n",
      "[ 0.12  0.13]\t\t[ 0.13  0.08  0.00  0.00  0.00  0.00]\t\t[-1.26  0.92]\n",
      "[ 0.22  0.18]\t\t[ 0.12  0.08  0.06  0.00  0.00  0.00]\t\t[ 2.12  1.03]\n",
      "[ 0.25  0.17]\t\t[ 0.12  0.06  0.06  0.00  0.00  0.00]\t\t[-1.52 -0.48]\n",
      "[ 0.35  0.10]\t\t[ 0.07  0.05  0.08  0.00  0.09  0.00]\t\t[ 1.27 -0.71]\n",
      "[-0.19 -0.02]\t\t[ 0.00  0.02  0.01  0.14  0.01  0.02]\t\t[ 0.44  0.77]\n",
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "\tdO obtained from the next layer in backprop\n",
      "[[ 0.94 -0.52]\n",
      " [ 0.1  -0.46]\n",
      " [-0.43 -0.31]\n",
      " [ 0.22 -0.48]\n",
      " [ 1.26 -0.89]\n",
      " [-0.19 -0.44]]\n",
      "\tdV \t\t=\t\t P.T \t\t\t@\t\t dO\n",
      "[ 1.98 -2.73]\t\t[ 1.00  1.00  1.00  1.00  0.77  0.02]\t\t[ 0.94 -0.52]\n",
      "[ 0.66 -1.53]\t\t[ 0.00  0.64  0.69  0.55  0.59  0.17]\t\t[ 0.10 -0.46]\n",
      "[ 1.16 -1.38]\t\t[ 0.00  0.00  0.46  0.51  0.92  0.09]\t\t[-0.43 -0.31]\n",
      "[-0.21 -0.49]\t\t[ 0.00  0.00  0.00  0.30  0.61  1.00]\t\t[ 0.22 -0.48]\n",
      "[ 1.39 -1.02]\t\t[ 0.00  0.00  0.00  0.00  1.00  0.04]\t\t[ 1.26 -0.89]\n",
      "[-0.02 -0.06]\t\t[ 0.00  0.00  0.00  0.00  0.00  0.11]\t\t[-0.19 -0.44]\n",
      "\tdP_dm \t\t\t\t=\t\t dO \t@\t\t\t V.T\n",
      "[ 1.45 -1.66  1.46 -1.18  1.55  0.02]\t\t[ 0.94 -0.52]\t\t[ 1.77 -1.26  2.12 -1.52  1.27  0.44]\n",
      "[-0.02 -0.55 -0.27  0.08  0.45 -0.32]\t\t[ 0.10 -0.46]\t\t[ 0.40  0.92  1.03 -0.48 -0.71  0.77]\n",
      "[-0.89  0.26 -1.24  0.81 -0.33 -0.43]\t\t[-0.43 -0.31]\t\t\t\n",
      "[ 0.20 -0.72 -0.02 -0.11  0.62 -0.27]\t\t[ 0.22 -0.48]\t\t\t\n",
      "[ 1.85 -2.40  1.74 -1.47  2.22 -0.14]\t\t[ 1.26 -0.89]\t\t\t\n",
      "[-0.51 -0.17 -0.85  0.50  0.07 -0.42]\t\t[-0.19 -0.44]\t\t\t\n",
      "\tdS \t\t\t\t=\t\t P \t\t\t*\t (dP - np.sum(P * dP))\n",
      "[ 0.00 -0.00  0.00 -0.00  0.00 -0.00]\t\t[ 1.00  0.00  0.00  0.00  0.00  0.00]\t\t[ 1.61]\n",
      "[ 0.39 -0.13  0.00  0.00  0.00  0.00]\t\t[ 1.00  0.64  0.00  0.00  0.00  0.00]\t\t[-0.41]\n",
      "[ 0.44  1.19  0.02  0.00  0.00  0.00]\t\t[ 1.00  0.69  0.46  0.00  0.00  0.00]\t\t[-1.43]\n",
      "[ 0.45 -0.31  0.10  0.07  0.00 -0.00]\t\t[ 1.00  0.55  0.51  0.30  0.00  0.00]\t\t[-0.23]\n",
      "[-1.68 -4.07 -2.12 -2.58 -1.78 -0.00]\t\t[ 0.77  0.59  0.92  0.61  1.00  0.00]\t\t[ 4.25]\n",
      "[-0.01 -0.10 -0.12  0.18 -0.01 -0.10]\t\t[ 0.02  0.17  0.09  1.00  0.04  0.11]\t\t[ 0.38]\n",
      "\tdQ \t=\t\t\t dS \t\t\t@\t\t K\t*\t tau\n",
      "[ 0.00  0.00]\t\t[ 0.00 -0.00  0.00 -0.00  0.00 -0.00]\t\t[-2.03  0.19]\t\t 0.71\n",
      "[-0.50 -0.03]\t\t[ 0.39 -0.13  0.00  0.00  0.00  0.00]\t\t[-0.66  0.85]\t\t\t\n",
      "[-1.20  0.77]\t\t[ 0.44  1.19  0.02  0.00  0.00  0.00]\t\t[-0.79 -0.11]\t\t\t\n",
      "[-0.53 -0.09]\t\t[ 0.45 -0.31  0.10  0.07  0.00 -0.00]\t\t[ 0.50  0.87]\t\t\t\n",
      "[ 6.08 -3.67]\t\t[-1.68 -4.07 -2.12 -2.58 -1.78 -0.00]\t\t[-1.20 -0.33]\t\t\t\n",
      "[ 0.24  0.11]\t\t[-0.01 -0.10 -0.12  0.18 -0.01 -0.10]\t\t[-0.47 -0.65]\t\t\t\n",
      "\tdK \t=\t\t\t dS.T \t\t\t@\t\t Q\t*\t tau\n",
      "[-0.51  0.75]\t\t[ 0.00  0.39  0.44  0.45 -1.68 -0.01]\t\t[-0.47  1.09]\t\t 0.71\n",
      "[-0.72  2.46]\t\t[-0.00 -0.13  1.19 -0.31 -4.07 -0.10]\t\t[ 0.06 -1.08]\t\t\t\n",
      "[-0.31  0.95]\t\t[ 0.00  0.00  0.02  0.10 -2.12 -0.12]\t\t[-0.72  0.68]\t\t\t\n",
      "[ 0.15  1.28]\t\t[-0.00  0.00  0.00  0.07 -2.58  0.18]\t\t[-0.73  0.22]\t\t\t\n",
      "[-0.08  0.81]\t\t[ 0.00  0.00  0.00  0.00 -1.78 -0.01]\t\t[ 0.05 -0.65]\t\t\t\n",
      "[-0.15 -0.04]\t\t[-0.00  0.00  0.00 -0.00 -0.00 -0.10]\t\t[ 2.14  0.63]\t\t\t\n"
     ]
    }
   ],
   "source": [
    "tau = 1 / np.sqrt(d)\n",
    "p_drop = 0.1\n",
    "\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "\n",
    "    # Forward pass\n",
    "    print(\"\\nForward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    S = tau * Q @ K.T\n",
    "    print(f\"\\tS \\t\\t\\t\\t=\\t\\t Q \\t@\\t\\t K.T \\t\\t\\t\\t*\\t tau\")\n",
    "    print_matop(S, Q, K.T, np.array([tau]))\n",
    "\n",
    "    # Apply causal mask directly to S\n",
    "    S_cm = np.where(np.tril(np.ones_like(S), k=0) == 1, S, -np.inf)\n",
    "    print(f\"\\tmasked S\")\n",
    "    print(S_cm)\n",
    "\n",
    "    m = np.max(S_cm, axis=1, keepdims=True)\n",
    "    S_ = S_cm - m\n",
    "    P = np.exp(S_)\n",
    "    print(f\"softmax numerator P:\")\n",
    "    print(f\"\\tP \\t\\t\\t\\t<=exp\\t\\t\\t (S - m):\")\n",
    "    print_matop(P, S_, separator='\\t\\t')\n",
    "\n",
    "    l = np.sum(np.exp(P), axis=1, keepdims=True)\n",
    "\n",
    "    # Apply dropout to P\n",
    "    drop_mask = ((np.random.rand(*P.shape) > p_drop) / (1 - p_drop))\n",
    "    P_dm = P * drop_mask \n",
    "    print(f\"P after drop_out:\")\n",
    "    print(f\"\\tP_dm \\t\\t\\t<=drop_out\\t\\t\\t P \\t\\t\\tby\\t\\t\\t drop_mask:\")\n",
    "    print_matop(P_dm, P, drop_mask, separator='\\t\\t')\n",
    "\n",
    "    A = P_dm / l\n",
    "    print(f\"Attention Score A:\")\n",
    "    print(f\"\\t\\tA \\t\\t\\t=\\t\\t\\t\\t P_dm \\t\\t\\t/\\t sum(exp(S_cm - m))\")\n",
    "    print_matop(A, P_dm, l, separator='\\t\\t\\t')\n",
    "\n",
    "    O = A @ V\n",
    "    print(f\"output matrix O:\")\n",
    "    print(f\"\\tO \\t=\\t\\t\\t A \\t\\t\\t@\\t\\t V\")\n",
    "    print_matop(O, A, V)\n",
    "\n",
    "    # Backward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")    \n",
    "    print(\"#\" * 50)\n",
    "    print(f\"\\tdO obtained from the next layer in backprop\")\n",
    "    print(dO)\n",
    "\n",
    "    dV = P_dm.T @ dO\n",
    "    print(f\"\\tdV \\t\\t=\\t\\t P.T \\t\\t\\t@\\t\\t dO\")\n",
    "    print_matop(dV, P.T, dO)\n",
    "\n",
    "    dP_dm = dO @ V.T\n",
    "    print(f\"\\tdP_dm \\t\\t\\t\\t=\\t\\t dO \\t@\\t\\t\\t V.T\")\n",
    "    print_matop(dP_dm, dO, V.T)\n",
    "\n",
    "    dP = dP_dm * drop_mask\n",
    "\n",
    "    dS = P * (dP - np.sum(P * dP, axis=1, keepdims=True))\n",
    "    print(f\"\\tdS \\t\\t\\t\\t=\\t\\t P \\t\\t\\t*\\t (dP - np.sum(P * dP))\")\n",
    "    print_matop(dS, P, np.sum(P * dP, axis=1, keepdims=True))\n",
    "\n",
    "    dQ = dS @ K * tau\n",
    "    print(f\"\\tdQ \\t=\\t\\t\\t dS \\t\\t\\t@\\t\\t K\\t*\\t tau\")\n",
    "    print_matop(dQ, dS, K, np.array([tau]))\n",
    "\n",
    "    dK = dS.T @ Q * tau\n",
    "    print(f\"\\tdK \\t=\\t\\t\\t dS.T \\t\\t\\t@\\t\\t Q\\t*\\t tau\")\n",
    "    print_matop(dK, dS.T, Q, np.array([tau]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISG_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

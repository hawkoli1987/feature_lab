{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import math\n",
    "np.random.seed(42)\n",
    "\n",
    "# helper function\n",
    "def print_matop(*matrices, separator=\"\\t\\t\"):\n",
    "    # Find the maximum number of rows across all matrices\n",
    "    max_rows = max(len(matrix) for matrix in matrices)\n",
    "    \n",
    "    # Iterate over the rows by index up to the max number of rows\n",
    "    for i in range(max_rows):\n",
    "        formatted_rows = []\n",
    "        for matrix in matrices:\n",
    "            # Check if the matrix has a row at index i\n",
    "            if i < len(matrix):\n",
    "                row = matrix[i]\n",
    "                # If row is iterable (like a list or numpy array), format each element\n",
    "                if hasattr(row, '__iter__'):\n",
    "                    formatted_rows.append(\"[\" + \" \".join(f\"{val:2.2f}\" for val in row) + \"]\")\n",
    "                else:\n",
    "                    # If it's a single scalar value, format it directly\n",
    "                    formatted_rows.append(f\"{row:2.2f}\")\n",
    "            else:\n",
    "                # Add an empty value if the matrix does not have enough rows\n",
    "                formatted_rows.append('\\t')\n",
    "        \n",
    "        # Join the formatted rows with the specified separator and print them\n",
    "        print(separator.join(formatted_rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bc: 3\n",
      "Br: 2\n",
      "Q \t\t K \t\t V\n",
      "[ 0.5  -0.14] \t [ 0.24 -1.91] \t [-0.54  0.11]\n",
      "[0.65 1.52] \t [-1.72 -0.56] \t [-1.15  0.38]\n",
      "[-0.23 -0.23] \t [-1.01  0.31] \t [-0.6  -0.29]\n",
      "[1.58 0.77] \t [-0.91 -1.41] \t [-0.6   1.85]\n",
      "[-0.47  0.54] \t [ 1.47 -0.23] \t [-0.01 -1.06]\n",
      "[-0.46 -0.47] \t [ 0.07 -1.42] \t [ 0.82 -1.22]\n"
     ]
    }
   ],
   "source": [
    "# SRAM memory size\n",
    "M = 20\n",
    "# simplified head size = hidden size\n",
    "d = 2\n",
    "# Sequence length\n",
    "N = 6\n",
    "\n",
    "# set block size for outer loop\n",
    "Bc = math.ceil(M / (4 * d))\n",
    "print(f\"Bc: {Bc}\")\n",
    "# set block size for inner loop\n",
    "Br = min(Bc, d)\n",
    "print(f\"Br: {Br}\")\n",
    "\n",
    "# Example dimensions for Q, K, V matrices\n",
    "Q = np.random.randn(N, d)\n",
    "K = np.random.randn(N, d)\n",
    "V = np.random.randn(N, d)\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Q \\t\\t K \\t\\t V\")\n",
    "    for q, k, v in zip(Q, K, V):\n",
    "        print(f\"{q} \\t {k} \\t {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Forward Pass 1\n",
    "- direct attention computation using numpy/scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output O:\n",
      "[[-0.17 -0.33]\n",
      " [-0.22 -0.7 ]\n",
      " [-0.41  0.14]\n",
      " [-0.03 -0.97]\n",
      " [-0.6   0.07]\n",
      " [-0.47  0.29]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax  # Optional, can use numpy's method or scipy's\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    # Step 1: Calculate the dot product of Q and K.T (scores)\n",
    "    S = np.dot(Q, K.T)  # (N, d) @ (d, N) -> (N, N)\n",
    "    \n",
    "    # Step 2: Apply the softmax to the scores for each row\n",
    "    P = softmax(S, axis=1)  # Softmax along the rows\n",
    "    \n",
    "    # Step 3: Multiply the attention weights with the value matrix V\n",
    "    O = np.dot(P, V)  # (N, N) @ (N, d) -> (N, d)\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Calculate the attention output\n",
    "_O = attention(Q, K, V)\n",
    "\n",
    "# Print the result\n",
    "print(\"Attention Output O:\")\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(_O)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "### Simplified Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "S \t\t\t=\t\t\t Q \t\t@\t\t K.T\n",
      "[0.38 -0.78 -0.55 -0.26 0.76 0.23]\t\t[0.50 -0.14]\t\t[0.24 -1.72 -1.01 -0.91 1.47 0.07]\n",
      "[-2.76 -1.97 -0.18 -2.74 0.61 -2.13]\t\t[0.65 1.52]\t\t[-1.91 -0.56 0.31 -1.41 -0.23 -1.42]\n",
      "[0.39 0.54 0.16 0.54 -0.29 0.32]\t\t[-0.23 -0.23]\t\t\t\n",
      "[-1.09 -3.16 -1.36 -2.52 2.14 -0.99]\t\t[1.58 0.77]\t\t\t\n",
      "[-1.15 0.50 0.65 -0.34 -0.81 -0.80]\t\t[-0.47 0.54]\t\t\t\n",
      "[0.78 1.06 0.32 1.08 -0.57 0.63]\t\t[-0.46 -0.47]\t\t\t\n",
      "A \t\t\t=\t\t\t exp(S - m) \t\t\t/\t sum(exp(S - m))\n",
      "[0.22 0.07 0.09 0.12 0.32 0.19]\t\t\t[0.69 0.21 0.27 0.36 1.00 0.59]\t\t\t[3.13]\n",
      "[0.02 0.05 0.27 0.02 0.60 0.04]\t\t\t[0.03 0.08 0.46 0.04 1.00 0.07]\t\t\t[1.67]\n",
      "[0.18 0.21 0.14 0.21 0.09 0.17]\t\t\t[0.86 0.99 0.68 1.00 0.43 0.80]\t\t\t[4.77]\n",
      "[0.04 0.00 0.03 0.01 0.89 0.04]\t\t\t[0.04 0.01 0.03 0.01 1.00 0.04]\t\t\t[1.13]\n",
      "[0.06 0.30 0.35 0.13 0.08 0.08]\t\t\t[0.17 0.87 1.00 0.37 0.23 0.23]\t\t\t[2.87]\n",
      "[0.18 0.24 0.12 0.25 0.05 0.16]\t\t\t[0.74 0.98 0.47 1.00 0.19 0.64]\t\t\t[4.03]\n",
      "O \t\t=\t\t\t A \t\t@\t\t V\n",
      "[-0.17 -0.33]\t\t[0.22 0.07 0.09 0.12 0.32 0.19]\t\t[-0.54 0.11]\n",
      "[-0.22 -0.70]\t\t[0.02 0.05 0.27 0.02 0.60 0.04]\t\t[-1.15 0.38]\n",
      "[-0.41 0.14]\t\t[0.18 0.21 0.14 0.21 0.09 0.17]\t\t[-0.60 -0.29]\n",
      "[-0.03 -0.97]\t\t[0.04 0.00 0.03 0.01 0.89 0.04]\t\t[-0.60 1.85]\n",
      "[-0.60 0.07]\t\t[0.06 0.30 0.35 0.13 0.08 0.08]\t\t[-0.01 -1.06]\n",
      "[-0.47 0.29]\t\t[0.18 0.24 0.12 0.25 0.05 0.16]\t\t[0.82 -1.22]\n",
      "dO obtained from the next layer in backprop\n",
      "[[ 0.21 -1.25]\n",
      " [ 0.17  0.39]\n",
      " [-0.88  0.15]\n",
      " [ 0.06 -1.14]\n",
      " [ 0.36  0.56]\n",
      " [ 1.08  1.05]]\n",
      "\n",
      "Backward Pass Outputs:\n",
      "##################################################\n",
      "dV \t\t=\t\t P.T \t\t@\t\t dO\n",
      "[0.26 0.12]\t\t[0.69 0.03 0.86 0.04 0.17 0.74]\t\t[0.21 -1.25]\n",
      "[0.56 1.43]\t\t[0.21 0.08 0.99 0.01 0.87 0.98]\t\t[0.17 0.39]\n",
      "[0.40 0.97]\t\t[0.27 0.46 0.68 0.03 1.00 0.47]\t\t[-0.88 0.15]\n",
      "[0.42 0.97]\t\t[0.36 0.04 1.00 0.01 0.37 1.00]\t\t[0.06 -1.14]\n",
      "[0.35 -1.60]\t\t[1.00 1.00 0.43 1.00 0.23 0.19]\t\t[0.36 0.56]\n",
      "[0.21 0.17]\t\t[0.59 0.07 0.80 0.04 0.23 0.64]\t\t[1.08 1.05]\n",
      "dP \t\t\t\t=\t\t dO \t\t@\t\t\t V.T\n",
      "[-0.25 -0.71 0.23 -2.44 1.31 1.70]\t\t[0.21 -1.25]\t\t[-0.54 -1.15 -0.60 -0.60 -0.01 0.82]\n",
      "[-0.05 -0.05 -0.22 0.61 -0.41 -0.33]\t\t[0.17 0.39]\t\t[0.11 0.38 -0.29 1.85 -1.06 -1.22]\n",
      "[0.50 1.08 0.49 0.82 -0.15 -0.91]\t\t[-0.88 0.15]\t\t\t\n",
      "[-0.16 -0.50 0.30 -2.15 1.21 1.44]\t\t[0.06 -1.14]\t\t\t\n",
      "[-0.13 -0.20 -0.38 0.82 -0.60 -0.39]\t\t[0.36 0.56]\t\t\t\n",
      "[-0.47 -0.85 -0.96 1.30 -1.13 -0.40]\t\t[1.08 1.05]\t\t\t\n",
      "dS \t\t\t\t=\t\t P \t\t\t*\t (dP - np.sum(P * dP))\n",
      "[-0.98 -0.40 -0.25 -1.31 0.15 0.31]\t\t[0.69 0.21 0.27 0.36 1.00 0.59]\t\t[1.17]\n",
      "[0.02 0.03 0.14 0.04 0.10 0.01]\t\t[0.03 0.08 0.46 0.04 1.00 0.07]\t\t[-0.51]\n",
      "[-1.16 -0.77 -0.93 -1.03 -0.87 -2.21]\t\t[0.86 0.99 0.68 1.00 0.43 0.80]\t\t[1.85]\n",
      "[-0.06 -0.01 -0.03 -0.03 -0.04 0.01]\t\t[0.04 0.01 0.03 0.01 1.00 0.04]\t\t[1.25]\n",
      "[0.06 0.26 0.12 0.49 -0.02 0.03]\t\t[0.17 0.87 1.00 0.37 0.23 0.23]\t\t[-0.50]\n",
      "[0.25 -0.04 -0.07 2.11 -0.06 0.26]\t\t[0.74 0.98 0.47 1.00 0.19 0.64]\t\t[-0.81]\n",
      "dQ \t\t=\t\t\t dS \t\t@\t\t K\n",
      "[2.14 3.38]\t\t[-0.98 -0.40 -0.25 -1.31 0.15 0.31]\t\t[0.24 -1.91]\n",
      "[-0.08 -0.10]\t\t[0.02 0.03 0.14 0.04 0.10 0.01]\t\t[-1.72 -0.56]\n",
      "[1.50 7.15]\t\t[-1.16 -0.77 -0.93 -1.03 -0.87 -2.21]\t\t[-1.01 0.31]\n",
      "[-0.00 0.15]\t\t[-0.06 -0.01 -0.03 -0.03 -0.04 0.01]\t\t[-0.91 -1.41]\n",
      "[-1.03 -0.95]\t\t[0.06 0.26 0.12 0.49 -0.02 0.03]\t\t[1.47 -0.23]\n",
      "[-1.78 -3.80]\t\t[0.25 -0.04 -0.07 2.11 -0.06 0.26]\t\t[0.07 -1.42]\n",
      "dK \t\t=\t\t\t dS.T \t\t@\t\t Q\n",
      "[-0.43 0.31]\t\t[-0.98 0.02 -1.16 -0.06 0.06 0.25]\t\t[0.50 -0.14]\n",
      "[-0.11 0.44]\t\t[-0.40 0.03 -0.77 -0.01 0.26 -0.04]\t\t[0.65 1.52]\n",
      "[0.11 0.54]\t\t[-0.25 0.14 -0.93 -0.03 0.12 -0.07]\t\t[-0.23 -0.23]\n",
      "[-1.64 -0.26]\t\t[-1.31 0.04 -1.03 -0.03 0.49 2.11]\t\t[1.58 0.77]\n",
      "[0.32 0.33]\t\t[0.15 0.10 -0.87 -0.04 -0.02 -0.06]\t\t[-0.47 0.54]\n",
      "[0.56 0.39]\t\t[0.31 0.01 -2.21 0.01 0.03 0.26]\t\t[-0.46 -0.47]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=2, suppress=True):\n",
    "\n",
    "    # Forward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    S = Q @ K.T\n",
    "    print(f\"S \\t\\t\\t=\\t\\t\\t Q \\t\\t@\\t\\t K.T\")\n",
    "    print_matop(S, Q, K.T)\n",
    "\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    P = np.exp(S - m)\n",
    "    l = np.sum(np.exp(S - m), axis=1, keepdims=True)\n",
    "    A = P / l\n",
    "    print(f\"A \\t\\t\\t=\\t\\t\\t exp(S - m) \\t\\t\\t/\\t sum(exp(S - m))\")\n",
    "    print_matop(A, P, l, separator='\\t\\t\\t')\n",
    "\n",
    "    _O = A @ V\n",
    "    print(f\"O \\t\\t=\\t\\t\\t A \\t\\t@\\t\\t V\")\n",
    "    print_matop(_O, A, V)\n",
    "    \n",
    "    dO = np.random.randn(N, d)  # Assume dO comes from the next layer\n",
    "    print(f\"dO obtained from the next layer in backprop\")\n",
    "    print(dO)\n",
    "\n",
    "    # Backward pass\n",
    "    print(\"\\nBackward Pass Outputs:\")\n",
    "    print(\"#\" * 50)\n",
    "    dV = P.T @ dO\n",
    "    print(f\"dV \\t\\t=\\t\\t P.T \\t\\t\\t@\\t dO\")\n",
    "    print_matop(dV, P.T, dO)\n",
    "\n",
    "    dP = dO @ V.T\n",
    "    print(f\"dP \\t\\t\\t\\t=\\t\\t dO \\t\\t@\\t\\t\\t V.T\")\n",
    "    print_matop(dP, dO, V.T)\n",
    "\n",
    "    dS = P * (dP - np.sum(P * dP, axis=1, keepdims=True))\n",
    "    print(f\"dS \\t\\t\\t\\t=\\t\\t P \\t\\t\\t\\t* (dP - np.sum(P * dP))\")\n",
    "    print_matop(dS, P, np.sum(P * dP, axis=1, keepdims=True))\n",
    "\n",
    "    dQ = dS @ K\n",
    "    print(f\"dQ \\t\\t=\\t\\t\\t dS \\t\\t\\t@\\t K\")\n",
    "    print_matop(dQ, dS, K)\n",
    "\n",
    "    dK = dS.T @ Q\n",
    "    print(f\"dK \\t\\t=\\t\\t\\t dS.T \\t\\t\\t@\\t Q\")\n",
    "    print_matop(dK, dS.T, Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass Output (O):\n",
      "[[-1.32023321        -inf        -inf        -inf]\n",
      " [-1.71313453  1.35387237        -inf        -inf]\n",
      " [-1.59442766 -0.59937502  0.0052437         -inf]\n",
      " [-0.45006547  0.62284993 -1.06762043 -0.14237949]]\n",
      "\n",
      "Backward Pass Output (dI):\n",
      "[[ 0.12029563  0.          0.         -0.        ]\n",
      " [-1.53411417  1.27767682  0.         -0.        ]\n",
      " [ 1.55115198  0.11567463  1.17929718  0.        ]\n",
      " [ 2.06074792  1.75534084 -0.24896415  0.97157095]]\n"
     ]
    }
   ],
   "source": [
    "def causal_mask(I):\n",
    "    mask = np.tril(np.ones_like(I), k=0)  # Lower triangular matrix with ones\n",
    "    O = np.where(mask == 0, -np.inf, I)   # Apply the mask\n",
    "    return O, mask\n",
    "\n",
    "def d_causal_mask(dO, mask):\n",
    "    dI = dO * mask  # Propagate gradient only through unmasked elements (mask == 1)\n",
    "    return dI\n",
    "\n",
    "I = np.random.randn(4, 4)\n",
    "\n",
    "# Forward pass\n",
    "O, mask = causal_mask(I)\n",
    "print(\"Forward Pass Output (O):\")\n",
    "print(O)\n",
    "\n",
    "# Assume some gradient coming from the next layer\n",
    "dO = np.random.randn(*O.shape)\n",
    "\n",
    "# Backward pass\n",
    "dI = d_causal_mask(dO, mask)\n",
    "print(\"\\nBackward Pass Output (dI):\")\n",
    "print(dI)\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\n",
      "[[ 1.68714164  0.88163976 -0.00797264  1.47994414]\n",
      " [ 0.07736831 -0.8612842   1.52312408  0.53891004]\n",
      " [-1.03724615 -0.19033868 -0.87561825 -1.38279973]\n",
      " [ 0.92617755  1.90941664 -1.39856757  0.56296924]]\n",
      "O:\n",
      "[[0.40928237 0.18289339 0.07513534 0.3326889 ]\n",
      " [0.13845177 0.05415604 0.58773596 0.21965624]\n",
      " [0.19172978 0.44719488 0.22536355 0.13571179]\n",
      " [0.22389644 0.59849772 0.02189895 0.15570689]]\n",
      "dO:\n",
      "[[-0.65064257 -0.48712538 -0.59239392 -0.86399077]\n",
      " [ 0.04852163 -0.83095012  0.27045683 -0.05023811]\n",
      " [-0.23894805 -0.90756366 -0.57677133  0.75539123]\n",
      " [ 0.50091719 -0.97755524  0.09933231  0.75138712]]\n",
      "dI:\n",
      "[[ 0.01501896  0.03661764  0.00713369 -0.05877028]\n",
      " [-0.00846181 -0.05093858  0.0945184  -0.03511801]\n",
      " [ 0.04605176 -0.1915893  -0.02200276  0.1675403 ]\n",
      " [ 0.19135461 -0.37335225  0.0099218   0.17207584]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(I: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    I: (n, m)\n",
    "    O: (n, m)\n",
    "    \"\"\"\n",
    "    exp_I = np.exp(I - np.max(I, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    O = exp_I / np.sum(exp_I, axis=1, keepdims=True)\n",
    "    return O\n",
    "\n",
    "def d_softmax(dO: np.ndarray, O: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    dO: (n, m)\n",
    "    O: (n, m)\n",
    "    dI: (n, m)\n",
    "    \"\"\"\n",
    "    n, m = O.shape\n",
    "    dI = np.zeros_like(dO)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Reshape O[i] to a column vector of shape (m, 1)\n",
    "        O_i = O[i].reshape(-1, 1)\n",
    "        # Compute the Jacobian matrix of the softmax function for this row, shape (m, m)\n",
    "        jacobian_i = np.diagflat(O_i) - np.dot(O_i, O_i.T)\n",
    "        # Compute the gradient for this row, shape (m,)\n",
    "        dI[i] = np.dot(jacobian_i, dO[i])\n",
    "    return dI\n",
    "\n",
    "# Example input matrix I of shape (4, 4)\n",
    "I = np.random.randn(4, 4)\n",
    "print(\"I:\")\n",
    "print(I)\n",
    "# Forward pass\n",
    "O = softmax(I)\n",
    "print(\"O:\")\n",
    "print(O)\n",
    "\n",
    "# Assume some gradient coming from the next layer, of shape (4, 4)\n",
    "dO = np.random.randn(*O.shape)\n",
    "print(\"dO:\")\n",
    "print(dO)\n",
    "\n",
    "# Backward pass\n",
    "dI = d_softmax(dO, O)\n",
    "print(\"dI:\")\n",
    "print(dI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISG_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
